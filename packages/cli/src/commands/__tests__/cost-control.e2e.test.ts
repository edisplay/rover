/**
 * ============================================================================
 * AUTOGENERATED E2E TEST FILE - DO NOT MODIFY MANUALLY
 * ============================================================================
 *
 * This file is automatically generated based on the E2E test specifications.
 * Any manual changes will be overwritten by the automation.
 *
 * To modify these tests, update the specification files:
 * - /docs/E2E_TESTS.md (root-level testing philosophy and constraints)
 * - /packages/cli/E2E_TESTS.md (CLI-specific test specifications)
 *
 * Then run the /update-e2e-tests command to regenerate this file.
 * ============================================================================
 */

import { beforeEach, afterEach, describe, it, expect } from 'vitest';
import { SKIP_REAL_AGENT_TESTS } from './e2e-utils.js';
import {
  mkdtempSync,
  rmSync,
  writeFileSync,
  readFileSync,
  existsSync,
  mkdirSync,
  chmodSync,
  readdirSync,
} from 'node:fs';
import { tmpdir } from 'node:os';
import { join } from 'node:path';
import { execa } from 'execa';

/**
 * E2E tests for Rover cost control
 *
 * These tests verify that Rover reports token usage and cost information
 * for workflow steps and overall workflow execution when the agent supports
 * reporting that information.
 *
 * Cost and token data is captured in the JSONL log files produced during
 * workflow execution. Step-level entries (event: "step_complete") include
 * optional `tokens` and `cost` fields. Workflow-level totals are the sum
 * of all step-level token/cost values.
 */

describe('rover cost control (e2e)', () => {
  let testDir: string;
  let originalCwd: string;
  let mockBinDir: string;
  let mockHomeDir: string;
  let originalPath: string;

  const createMockTool = (
    toolName: string,
    exitCode: number = 0,
    output: string = 'mock version 1.0.0'
  ) => {
    const scriptPath = join(mockBinDir, toolName);
    const scriptContent = `#!/usr/bin/env bash\necho "${output}"\nexit ${exitCode}`;
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  const createMockScript = (toolName: string, scriptContent: string) => {
    const scriptPath = join(mockBinDir, toolName);
    writeFileSync(scriptPath, scriptContent);
    chmodSync(scriptPath, 0o755);
  };

  const createMockClaude = () => {
    createMockScript(
      'claude',
      `#!/usr/bin/env bash
if [[ "$1" == "--version" ]]; then
  echo "Claude CLI v1.0.0"
  exit 0
fi

if [[ "$1" == "-p" ]]; then
  PROMPT=$(cat)
  if [[ "$2" == "--output-format" && "$3" == "json" ]]; then
    echo '{"result":"{\\"title\\":\\"Test task\\",\\"description\\":\\"A test task description.\\"}"}'
  else
    echo '{"title":"Test task","description":"A test task description."}'
  fi
  exit 0
fi

echo "Claude CLI v1.0.0"
exit 0
`
    );
  };

  const roverBin = join(__dirname, '../../../dist/index.mjs');

  const runRover = async (args: string[]) => {
    const testPath = `${mockBinDir}:${originalPath}`;
    return execa('node', [roverBin, ...args], {
      cwd: testDir,
      env: {
        PATH: testPath,
        HOME: mockHomeDir,
        USER: process.env.USER,
        TMPDIR: process.env.TMPDIR,
        ROVER_NO_TELEMETRY: '1',
      },
      reject: false,
    });
  };

  const waitForTaskCompletion = async (
    taskId: number,
    timeoutMs: number = 600000
  ): Promise<string> => {
    const startTime = Date.now();
    while (Date.now() - startTime < timeoutMs) {
      const result = await runRover(['inspect', String(taskId), '--json']);
      if (result.exitCode === 0) {
        const output = JSON.parse(result.stdout);
        if (output.status === 'COMPLETED' || output.status === 'FAILED')
          return output.status;
      }
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    throw new Error(`Timeout waiting for task ${taskId} to complete`);
  };

  /**
   * Finds and reads the JSONL log file for a given task and iteration.
   * Returns parsed log entries.
   */
  const readJsonlLog = (taskId: number): object[] => {
    const taskDir = join(testDir, `.rover/tasks/${taskId}`);
    if (!existsSync(taskDir)) return [];

    // Search for rover.jsonl in iteration directories
    const entries: object[] = [];
    const iterationsDir = join(taskDir, 'iterations');
    if (existsSync(iterationsDir)) {
      const iterDirs = readdirSync(iterationsDir);
      for (const dir of iterDirs) {
        const jsonlPath = join(iterationsDir, dir, 'rover.jsonl');
        if (existsSync(jsonlPath)) {
          const content = readFileSync(jsonlPath, 'utf8');
          for (const line of content.split('\n').filter(l => l.trim())) {
            try {
              entries.push(JSON.parse(line));
            } catch {
              // Skip malformed lines
            }
          }
        }
      }
    }

    // Also check the task root for rover.jsonl
    const rootJsonl = join(taskDir, 'rover.jsonl');
    if (existsSync(rootJsonl)) {
      const content = readFileSync(rootJsonl, 'utf8');
      for (const line of content.split('\n').filter(l => l.trim())) {
        try {
          entries.push(JSON.parse(line));
        } catch {
          // Skip malformed lines
        }
      }
    }

    return entries;
  };

  /**
   * Workflow YAML with agent steps for cost control testing
   */
  const costControlWorkflowYaml = `name: cost-control-test
description: A workflow for testing cost control reporting
version: "1.0"
steps:
  - id: analyze
    name: Analyze
    type: agent
    description: Analyze the codebase
    prompt: "Analyze the codebase and list the files present"
  - id: implement
    name: Implement
    type: agent
    description: Implement a simple change
    prompt: "Create a file called hello.txt with the text Hello World"
`;

  beforeEach(async () => {
    originalCwd = process.cwd();
    originalPath = process.env.PATH || '';

    testDir = mkdtempSync(join(tmpdir(), 'rover-cost-control-e2e-'));
    process.chdir(testDir);

    mockBinDir = join(testDir, '.mock-bin');
    mkdirSync(mockBinDir, { recursive: true });

    // Create mock HOME directory with agent configuration files
    mockHomeDir = join(testDir, '.mock-home');
    mkdirSync(mockHomeDir, { recursive: true });
    writeFileSync(
      join(mockHomeDir, '.claude.json'),
      JSON.stringify({ version: 1 })
    );

    process.env.PATH = `${mockBinDir}:${originalPath}`;

    createMockTool('docker', 127, 'command not found: docker');
    createMockTool('claude', 127, 'command not found: claude');
    createMockTool('codex', 127, 'command not found: codex');
    createMockTool('cursor', 127, 'command not found: cursor');
    createMockTool('cursor-agent', 127, 'command not found: cursor-agent');
    createMockTool('gemini', 127, 'command not found: gemini');
    createMockTool('qwen', 127, 'command not found: qwen');
    createMockTool('opencode', 127, 'command not found: opencode');

    createMockTool('docker', 0, 'Docker version 24.0.0');
    createMockClaude();

    await execa('git', ['init']);
    await execa('git', ['config', 'user.email', 'test@test.com']);
    await execa('git', ['config', 'user.name', 'Test User']);
    await execa('git', ['config', 'commit.gpgsign', 'false']);

    writeFileSync(
      'package.json',
      JSON.stringify(
        { name: 'test-project', version: '1.0.0', type: 'module' },
        null,
        2
      )
    );
    writeFileSync('README.md', '# Test Project\n');

    await execa('git', ['add', '.']);
    await execa('git', ['commit', '-m', 'Initial commit']);

    await runRover(['init', '--yes']);
  });

  afterEach(() => {
    process.chdir(originalCwd);
    process.env.PATH = originalPath;
    rmSync(testDir, { recursive: true, force: true });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Step token/cost usage tests require a real agent that supports
   *   reporting token usage and cost via ACP usage_update events.
   *   With mock Docker, no real agent execution occurs, so no
   *   token/cost data is produced.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Ensure Docker is running and can pull the rover agent image
   *   3. Ensure a valid AI agent (Claude CLI, Gemini CLI, etc.) is installed and authenticated
   *   4. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "step token"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)('step token and/or cost usage', () => {
    it('should report token consumption for each agent step in a workflow', async () => {
      // Add the cost control test workflow
      const workflowFile = join(testDir, 'cost-control-workflow.yml');
      writeFileSync(workflowFile, costControlWorkflowYaml);
      await runRover(['workflows', 'add', workflowFile, '--json']);

      // Run a task with the workflow
      const taskResult = await runRover([
        'task',
        '-y',
        'Test cost control reporting',
        '--workflow',
        'cost-control-test',
        '--json',
      ]);
      expect(taskResult.exitCode).toBe(0);

      // Wait for the task to complete
      await waitForTaskCompletion(1);

      // Read the JSONL log to find step_complete entries
      const logEntries = readJsonlLog(1);
      const stepCompleteEntries = logEntries.filter(
        (entry: any) => entry.event === 'step_complete'
      );

      // There should be at least one step_complete entry
      expect(stepCompleteEntries.length).toBeGreaterThanOrEqual(1);

      // At least one step should have token information
      const stepsWithTokens = stepCompleteEntries.filter(
        (entry: any) => typeof entry.tokens === 'number' && entry.tokens > 0
      );
      expect(stepsWithTokens.length).toBeGreaterThanOrEqual(1);
    });

    it('should report cost for each agent step when the agent supports cost reporting', async () => {
      // Add the cost control test workflow
      const workflowFile = join(testDir, 'cost-control-workflow.yml');
      writeFileSync(workflowFile, costControlWorkflowYaml);
      await runRover(['workflows', 'add', workflowFile, '--json']);

      // Run a task with the workflow
      const taskResult = await runRover([
        'task',
        '-y',
        'Test cost control reporting',
        '--workflow',
        'cost-control-test',
        '--json',
      ]);
      expect(taskResult.exitCode).toBe(0);

      // Wait for the task to complete
      await waitForTaskCompletion(1);

      // Read the JSONL log to find step_complete entries
      const logEntries = readJsonlLog(1);
      const stepCompleteEntries = logEntries.filter(
        (entry: any) => entry.event === 'step_complete'
      );

      // There should be at least one step_complete entry
      expect(stepCompleteEntries.length).toBeGreaterThanOrEqual(1);

      // If the agent supports cost reporting, at least one step should have cost data
      const stepsWithCost = stepCompleteEntries.filter(
        (entry: any) => typeof entry.cost === 'number' && entry.cost > 0
      );

      // Cost reporting is optional (depends on agent support), but if present
      // it should be a positive number
      for (const step of stepsWithCost) {
        expect((step as any).cost).toBeGreaterThan(0);
      }
    });

    it('should include model information for each agent step', async () => {
      // Add the cost control test workflow
      const workflowFile = join(testDir, 'cost-control-workflow.yml');
      writeFileSync(workflowFile, costControlWorkflowYaml);
      await runRover(['workflows', 'add', workflowFile, '--json']);

      // Run a task with the workflow
      const taskResult = await runRover([
        'task',
        '-y',
        'Test cost control reporting',
        '--workflow',
        'cost-control-test',
        '--json',
      ]);
      expect(taskResult.exitCode).toBe(0);

      // Wait for the task to complete
      await waitForTaskCompletion(1);

      // Read the JSONL log to find step_complete entries
      const logEntries = readJsonlLog(1);
      const stepCompleteEntries = logEntries.filter(
        (entry: any) => entry.event === 'step_complete'
      );

      // At least one step should include the model used
      const stepsWithModel = stepCompleteEntries.filter(
        (entry: any) =>
          typeof entry.model === 'string' && entry.model.length > 0
      );
      expect(stepsWithModel.length).toBeGreaterThanOrEqual(1);
    });
  });

  /**
   * SKIPPED: Tests require real AI agent execution
   *
   * Why skipped:
   *   Workflow token/cost usage tests require a complete workflow execution
   *   with a real agent that supports reporting token usage and cost.
   *   With mock Docker, no real agent execution occurs, so no
   *   token/cost data is produced.
   *
   * TODO: To unskip these tests:
   *   1. Set ROVER_E2E_REAL_AGENT=true environment variable
   *   2. Ensure Docker is running and can pull the rover agent image
   *   3. Ensure a valid AI agent (Claude CLI, Gemini CLI, etc.) is installed and authenticated
   *   4. Run with: ROVER_E2E_REAL_AGENT=true pnpm e2e-test --grep "workflow token"
   */
  describe.skipIf(SKIP_REAL_AGENT_TESTS)(
    'workflow token and/or cost usage',
    () => {
      it('should report total token usage across all steps in a workflow', async () => {
        // Add the cost control test workflow
        const workflowFile = join(testDir, 'cost-control-workflow.yml');
        writeFileSync(workflowFile, costControlWorkflowYaml);
        await runRover(['workflows', 'add', workflowFile, '--json']);

        // Run a task with the workflow
        const taskResult = await runRover([
          'task',
          '-y',
          'Test cost control reporting',
          '--workflow',
          'cost-control-test',
          '--json',
        ]);
        expect(taskResult.exitCode).toBe(0);

        // Wait for the task to complete
        await waitForTaskCompletion(1);

        // Read the JSONL log to find step_complete entries
        const logEntries = readJsonlLog(1);
        const stepCompleteEntries = logEntries.filter(
          (entry: any) => entry.event === 'step_complete'
        );

        // Calculate total tokens across all steps
        const totalTokens = stepCompleteEntries.reduce(
          (sum: number, entry: any) => sum + (entry.tokens || 0),
          0
        );

        // The total token usage across the workflow should be positive
        expect(totalTokens).toBeGreaterThan(0);

        // Each individual step's tokens should be a non-negative number
        for (const entry of stepCompleteEntries) {
          if ((entry as any).tokens !== undefined) {
            expect((entry as any).tokens).toBeGreaterThanOrEqual(0);
          }
        }
      });

      it('should report total cost across all steps in a workflow when the agent supports cost reporting', async () => {
        // Add the cost control test workflow
        const workflowFile = join(testDir, 'cost-control-workflow.yml');
        writeFileSync(workflowFile, costControlWorkflowYaml);
        await runRover(['workflows', 'add', workflowFile, '--json']);

        // Run a task with the workflow
        const taskResult = await runRover([
          'task',
          '-y',
          'Test cost control reporting',
          '--workflow',
          'cost-control-test',
          '--json',
        ]);
        expect(taskResult.exitCode).toBe(0);

        // Wait for the task to complete
        await waitForTaskCompletion(1);

        // Read the JSONL log to find step_complete entries
        const logEntries = readJsonlLog(1);
        const stepCompleteEntries = logEntries.filter(
          (entry: any) => entry.event === 'step_complete'
        );

        // Calculate total cost across all steps
        const totalCost = stepCompleteEntries.reduce(
          (sum: number, entry: any) => sum + (entry.cost || 0),
          0
        );

        // If the agent supports cost reporting, total cost should be positive
        const stepsWithCost = stepCompleteEntries.filter(
          (entry: any) => typeof entry.cost === 'number' && entry.cost > 0
        );

        if (stepsWithCost.length > 0) {
          expect(totalCost).toBeGreaterThan(0);
        }

        // Each individual step's cost should be a non-negative number
        for (const entry of stepCompleteEntries) {
          if ((entry as any).cost !== undefined) {
            expect((entry as any).cost).toBeGreaterThanOrEqual(0);
          }
        }
      });

      it('should produce step_complete entries for each agent step in the workflow', async () => {
        // Add the cost control test workflow (has 2 agent steps)
        const workflowFile = join(testDir, 'cost-control-workflow.yml');
        writeFileSync(workflowFile, costControlWorkflowYaml);
        await runRover(['workflows', 'add', workflowFile, '--json']);

        // Run a task with the workflow
        const taskResult = await runRover([
          'task',
          '-y',
          'Test cost control reporting',
          '--workflow',
          'cost-control-test',
          '--json',
        ]);
        expect(taskResult.exitCode).toBe(0);

        // Wait for the task to complete
        await waitForTaskCompletion(1);

        // Read the JSONL log to find step_complete entries
        const logEntries = readJsonlLog(1);
        const stepCompleteEntries = logEntries.filter(
          (entry: any) => entry.event === 'step_complete'
        );

        // The workflow has 2 agent steps, so there should be at least 2 step_complete entries
        expect(stepCompleteEntries.length).toBeGreaterThanOrEqual(2);

        // Each step_complete entry should have a stepId and duration
        for (const entry of stepCompleteEntries) {
          expect((entry as any).stepId).toBeDefined();
          expect((entry as any).duration).toBeDefined();
        }
      });
    }
  );
});

/**
 * ============================================================================
 * AUTOGENERATED E2E TEST FILE - DO NOT MODIFY MANUALLY
 * ============================================================================
 *
 * To modify these tests, update the specification files:
 * - /docs/E2E_TESTS.md (root-level testing philosophy and constraints)
 * - /packages/cli/E2E_TESTS.md (CLI-specific test specifications)
 *
 * Then run the /update-e2e-tests command to regenerate this file.
 * ============================================================================
 */
